<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>The Human-Baxter Collaboration Framework: Introducing the Human-Baxter Collaboration Framework</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">The Human-Baxter Collaboration Framework
   &#160;<span id="projectnumber">v2.0</span>
   </div>
   <div id="projectbrief">The Human-Baxter collaboration framework is a modular, easy to modify and adapt framework for collaborative experiments with human collaborators and a Baxter research robot.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Introducing the Human-Baxter Collaboration Framework </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This repository contains <a href="https://brml.org/brml/">BRML</a>'s Human-Baxter collaboration framework. It was implemented as part of the <a href="http://dfki.de/smartfactories/">EIT CPS for Smart Factories project</a>, in collaboration with the <a href="http://nipg.inf.elte.hu/">Neural Information Processing Group</a> at ELTE, Budapest, Hungary and <a href="http://dfki.de/web">DFKI</a> Saarbruecken, Germany.</p>
<h2>Overview</h2>
<p>The Human-Baxter collaboration framework aims at being a modular, easy to modify and adapt, framework for collaborative experiments with human collaborators and a <a href="http://www.rethinkrobotics.com/research-education/">Baxter research robot</a>.</p>
<p>The distributed pick-and-place scenario integrates the Baxter robot, the Microsoft Kinect V2 sensor and deep neural networks to detect, pick up and place objects. Three types of experiments are possible:</p><ul>
<li>picking and placing an object on a table,</li>
<li>handing over an object to a human collaborator and</li>
<li>taking over an object from a human collaborator.</li>
</ul>
<h2>Dependencies and Requirements</h2>
<p>Two possibilities to use the Human-Baxter collaboration framework exist. You either need the open source <a href="http://sdk.rethinkrobotics.com/wiki/Baxter_Simulator">Baxter simulator</a> or access to a <a href="http://www.rethinkrobotics.com/research-education/">Baxter research robot</a>.</p>
<p>The Kinect V2 can be interfaced either on Ubuntu/ROS via the <a href="https://github.com/code-iai/iai_kinect2">iai_kinect2</a> package (no skeleton tracking) or via a web socket connection and the ELTE Kinect Windows tool running on a Windows machine.</p>
<p>The framework heavily builds upon the <a href="https://github.com/RethinkRobotics">Baxter SDK</a> and depends on customized versions of <a href="https://github.com/BRML/baxter_interface.git">baxter_interface</a> and <a href="https://github.com/BRML/baxter_common.git">baxter_common</a> from <a href="http://www.rethinkrobotics.com/">Rethink Robotics</a>. For the simulation in Gazebo, the <a href="https://github.com/BRML/depth_sensors.git">depth_sensors</a> package is utilized. For image processing we rely on the <a href="http://opencv.org/">OpenCV</a> library.</p>
<p>The framework has been tested with ROS Indigo on Ubuntu 14.04. For the simulator running in <a href="http://gazebosim.org/">Gazebo</a> a machine with (any) NVIDIA GPU has been proven useful. For the object detection using <a href="https://github.com/rbgirshick/py-faster-rcnn">faster R-CNN</a> a GPU with at least 2GB RAM is required. For the object detection using <a href="https://github.com/Orpine/py-R-FCN">R-FCN</a> and object segmentation using <a href="https://github.com/daijifeng001/MNC">MNC</a> a GPU with at least 7 GB RAM are required. We made good experiences with NVIDIA Quadro K2200 and NVIDIA TITAN X GPUs.</p>
<h2>License</h2>
<p>We publish the Human-Baxter collaboration framework under a BSD license, hoping that it might be useful for others as well. The license text can be found in the LICENSE file and can be obtained from the <a href="https://opensource.org/licenses/BSD-2-Clause">Open Source Initiative</a>.</p>
<p>If you find our Human-Baxter collaboration framework useful in your work, please consider citing it: </p><div class="fragment"><div class="line">@misc{hbcf2016,</div><div class="line">    author={Ludersdorfer, Marvin},</div><div class="line">    title={{The Human-Baxter collaboration framework}},</div><div class="line">    organization={{Biomimetic Robotics and Machine Learning laboratory}},</div><div class="line">    address={{fortiss GmbH}},</div><div class="line">    year={2015--2016},</div><div class="line">    howpublished={\url{https://github.com/BRML/baxter\_pick\_and\_place}},</div><div class="line">    note={Accessed November 30, 2016}</div><div class="line">}</div></div><!-- fragment --><h2>Installation</h2>
<p>The framework is implemented as and uses several <a href="http://www.ros.org/">ROS</a> packages that can be installed conveniently as described <a class="el" href="md_install.html">here</a>.</p>
<h2>Usage</h2>
<p>How to run the distributed pick-and-place scenario is explained in detail here.</p>
<h2>Acknowledgements</h2>
<p>We thank Aron Fothi, Mike Olasz, Andras Sarkany and Zoltan Toser from the <a href="http://nipg.inf.elte.hu/">Neural Information Processing Group</a> at ELTE for their help and many valuable discussions.</p>
<h2>Known Limitations and Bugs</h2>
<ul>
<li>No Gazebo models of objects to manipulate are included.</li>
<li>The external calibration routine gives rather poor results. We can handle this, since the visual servoing is able to compensate for coarse position estimates. </li>
</ul>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.12
</small></address>
</body>
</html>
